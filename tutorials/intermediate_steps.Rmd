---
title: 'More particles or more intermediate steps?'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question

In this script, we consider two ways of improving the precision of estimators generated by sequential Monte Carlo samplers (SMCS):

* increasing the number of particles;

* increasing the number of intermediate steps.

Does the second choice bring any benefits compared to the first?

We will see that the latter can be preferable in terms of precision per compute time. On top of that it is also preferable in terms of memory usage. Therefore it is a useful alternative to keep in mind for users wanting to improve the quality of SMCS results.

## Setup

We first load the `smcsamplers` package, some other packages and register some parallel cores.

```{r loadpackage, message=F, warning=F}
rm(list = ls())
# install.packages("devtools")
# devtools::install_github("pierrejacob/smcsamplers")
library(smcsamplers)
library(doParallel)
library(doRNG)
library(tidyverse)
cores <- max(1, detectCores()-2)
registerDoParallel(cores = cores)
set.seed(3)
## number of independent repetitions
nrep <- 50
```

We consider the task of approximating the normalizing constant of Normal$(0_d,I_d)$, starting from Normal$(1_d,0.5 I_d)$ in dimension $d$, where $0_d$ and $1_d$ are vectors of zeros and ones respectively, and $I_d$ is the $d$-dimensional identity matrix.


```{r targetdef}
## dimension of state space
dimension <- 64
## initial distribution "pi_0"
initmean <- rep(1, dimension)
initvar <- rep(0.5, dimension)
initdist <- get_mvnormal_diag(initmean, initvar)
## target distribution "pi"
targetmean <- rep(0, dimension)
targetvar <- rep(1, dimension)
targetdist <- get_mvnormal_diag(targetmean, targetvar)
```

We tune an adaptive SMCS algorithm, using a geometric path between $\pi_0$ and $\pi$. That is,
we define a path of distributions $(\pi_t)_{t=0}^T$ with 
$$\pi_t(x)=\frac{\gamma_t(x)}{Z_t},\quad 
\gamma_t(x)= \pi_0(x)^{1-\lambda_t} \pi(x)^{\lambda_t}, \quad
Z_t=\int_{\mathbb{R}^d}\gamma_t(x)dx,
$$ 
defined for a sequence $0=\lambda_0<\lambda_1<\cdots<\lambda_T=1$. 
The algorithm determines a sequence of intermediate distributions based on the Effective Sample Size (ESS) criterion. For the move steps we will use Hamiltonian Monte Carlo, which is known to perform well for
Normal distributions and other similar ones.

```{r first_tuning}
## set tuning parameters for adaptive SMC sampler with HMC moves
smctuning <- list()
## number of particles
smctuning$nparticles <- 2^9
## ESS criterion threshold; number in (0,1)
smctuning$ess_criterion <- 0.5
## stepsize for the HMC moves
smctuning$stepsize <- 1 * dimension^{-1/4}
## number of leapfrog steps
smctuning$nleapfrog <- ceiling(1/smctuning$stepsize)
## number of HMC moves to perform at each move step
smctuning$nmoves <- 1
```

We run the adaptive SMCS algorithm and look at some of its output.

```{r first_run}
## 
initparticles <- initmean + sqrt(initvar) * matrix(rnorm(dimension*smctuning$nparticles), nrow = dimension)
results <- asmc_hmc(smctuning, targetdist, initdist, initparticles)
nsteps <- length(results$xhistory)
cat("Adaptive SMCS has determined", nsteps, "intermediate steps.\n")
plot(2:nsteps, results$ess_realized * 100 / smctuning$nparticles, xlab = 'step', ylab = 'ESS%', ylim = c(0, 100), main = "ESS% along the steps")
abline(h = smctuning$ess_criterion * smctuning$nparticles, lty = 2)
plot(sapply(results$infos_mcmc, function(x) x[[1]]$ar), ylim = c(0,1), xlab = 'step', ylab = 'HMC acceptance rate', main = "acceptance rate along the steps")
plot(1:nsteps, results$lambdas, ylim = c(0,1), xlab = 'step', ylab = 'lambda', main = 'lambda along the steps')
```

The adaptive SMCS run seems to have executed satisfactorily: the ESS
was reset whenever it reached the specified criterion,
the HMC moves led to high acceptance rates, and in total `r nsteps` intermediate
steps were selected.

Now we can run the algorithm `r nrep` times independently 
and look at the variance of $\log {Z}_T^N$, the estimator 
of the logarithm of the normalizing constant ratio $Z/Z_0$. Note that here
$Z_0=Z=1$ so that ${Z}_T^N$ is an estimator of one.

```{r originalruns}
smctuning$lambdas <- results$lambdas
zhat_original_df <- foreach(irep = 1:nrep, .combine = rbind) %dorng% {
  initparticles <- initmean + sqrt(initvar) * matrix(rnorm(dimension*smctuning$nparticles), nrow = dimension)
  results <- smc_hmc(smctuning, targetdist, initdist, initparticles)
  nsteps <- length(results$lambdas)
  zhat <- sum(results$log_ratio_normconst)
  data.frame(nsteps = nsteps, zhat = zhat, rep = irep, nparticles = smctuning$nparticles)
}
print(var(zhat_original_df$zhat))
```

If we want to reduce the variance of that estimator,
the most obvious solution is to increase the number of particles.

## Increasing the number of particles

We double the number of particles and see the impact on the variance of $\log {Z}_T^N$.

```{r moreparticles}
smctuning_moreparticles <- smctuning
smctuning_moreparticles$nparticles <- 2*smctuning$nparticles

zhat_moreparticles_df <- foreach(irep = 1:nrep, .combine = rbind) %dorng% {
  initparticles <- initmean + sqrt(initvar) * matrix(rnorm(dimension*smctuning_moreparticles$nparticles), nrow = dimension)
  results <- smc_hmc(smctuning_moreparticles, targetdist, initdist, initparticles)
  nsteps <- length(results$lambdas)
  zhat <- sum(results$log_ratio_normconst)
  data.frame(nsteps = nsteps, zhat = zhat, rep = irep, nparticles = smctuning_moreparticles$nparticles)
}
var(zhat_moreparticles_df$zhat)
```

Indeed this works as expected: the variance is approximately halved when the number of particles
is doubled.

## Increasing the number of intermediate steps

An alternative route to obtain more precise results
is to increase the number of intermediate steps.
For example we can double that number, which results in an approximate doubling
of the computing cost.
To add intermediate values of $\lambda$
we can perform a parametric approximation of $\lambda_t$ against $t$,
and add more $t$'s. Here we use the `cobs` package to 
fit splines constrained to be increasing.

```{r gridrefinement, message=F, warning=F, echo=F}
## finer grid of lambdas
nintermediate <- nsteps*2
nlambdas <- nintermediate
require(cobs)
xgrid <- (0:(nsteps-1))/(nsteps-1)
fit = cobs(x = xgrid, y = results$lambdas, constraint= "increase",
           lambda=0, degree=1, # for L1 roughness
           knots=seq(0, 1, length.out=floor(nsteps/2)), # desired nr of knots
           tau=0.5, print.warn = F, print.mesg = F) # to predict median
xfinergrid <- seq(from = 0, to = 1, length.out = nlambdas)
yrange <- predict(fit,interval="none",z=c(0,1))[,2]
yfinergrid <- (predict(fit,interval="none",z=xfinergrid)[,2] - yrange[1])/(yrange[2]-yrange[1])
plot(xgrid, results$lambdas, ylim = c(0,1), xlab = 'step in (0,1)', ylab = 'lambda')
points(xfinergrid, yfinergrid, col = 'red', lty = 2)
legend(x = 0, y = 0.8, legend = c('original schedule', 'refined schedule'), col = c('black', 'red'), lty = c(1,1))
```

Next we run SMCS with the 'refined' schedule for $(\lambda_t)$.

```{r runwithmorelambdas}
## set up grid of lambdas
smctuning_morelambdas <- smctuning
smctuning_morelambdas$lambdas <- yfinergrid
smctuning_morelambdas$lambdas <- pmax(pmin(smctuning_morelambdas$lambdas, 1), 0)
zhat_morelambdas_df <- foreach(irep = 1:nrep, .combine = rbind) %dorng% {
  initparticles <- initmean + sqrt(initvar) * matrix(rnorm(dimension*smctuning_morelambdas$nparticles), nrow = dimension)
  results <- smc_hmc(smctuning_morelambdas, targetdist, initdist, initparticles)
  nsteps <- length(results$lambdas)
  zhat <- sum(results$log_ratio_normconst)
  data.frame(nsteps = nsteps, zhat = zhat, rep = irep, nparticles = smctuning_morelambdas$nparticles)
}
#
var(zhat_morelambdas_df$zhat)
```

## Conclusion

Overall we have the following variances of $\log {Z}_T^N$ for the different experiments,
starting with an adaptive SMCS run with `r smctuning$nparticles` particles that
selected `r nsteps` intermediate steps.

```{r comparevar}
cat("Original adaptive SMCS:", var(zhat_original_df$zhat), '\n')
cat("Same intermediate steps and twice as many particles:", var(zhat_moreparticles_df$zhat), '\n')
cat("Same number of particles but twice as many intermediate steps:", var(zhat_morelambdas_df$zhat), '\n')
```

We see that both strategies reduced the variance. Both can be considered
as possible ways of improving the performance of SMCS.
