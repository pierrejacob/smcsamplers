---
title: 'Logistic regression: Geometric path'
output: html_document
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data set and prior specification
We begin by loading the forest cover type data set [@blackard2000comparison] which has been pre-processed. The data contain cartographic information (relating to altitude, slope, azimuth etc) for $30m$ by $30m$ cells in northern Colorado. The goal is to predict the type of cover (originally spruce/fir, lodgepole pine, Ponderosa pine, cottonwood/willow, spruce/fir and aspen or Douglas-fir, and in @collobert2002parallel this was simplified to lodgepole pine versus the other categories combined). We will fit a Bayesian logistic regression model to analyze this data set with $m=1000$ observations. In the following, we denote $x=(x_1,\ldots,x_m)\in\mathbb{R}^{m\times d}$ as the design matrix and $y=(y_1,\ldots,y_m)\in\{0,1\}^m$ as the observations. 

```{r dataset}
load("covtype.processed.RData")
m <- 1e3 # number of training datapoints
d <- dim(trainingset)[2] - 1 # number of covariates
Y <- trainingset[1:m,1] - 1
X <- trainingset[1:m,2:(d+1)]
```

Next, we standardize the covariates and add a column of ones to the design matrix $x$ to account for the regression intercept.

```{r standardize}
X <- scale(X)
X <- cbind(1, X)
d <- d + 1
```

Let $\beta\in\mathbb{R}^d$ denote the regression coefficients. We implement a function `loglikelihood` to evaluate the log-likelihood under the logistic regression model
$$
\log p(y|x,\beta)=\sum_{i=1}^my_ix_i^\top\beta -\sum_{i=1}^m \log(1+\exp(\beta^\top x_i)),
$$
and a function `gradloglikelihood` to evaluate its gradient with respect to $\beta$ 
$$
\nabla \log p(y|x,\beta)=\sum_{i=1}^my_ix_i -\sum_{i=1}^m (1+\exp(-\beta^\top x_i))^{-1}x_i.
$$
These functions take as input a $d\times N$ matrix `beta` containing $N$ sets of regression coefficients. 

```{r likelihood}
loglikelihood <- function(beta){
  xbeta <- X %*% beta
  return(colSums(Y * xbeta) - colSums(log(1 + exp(xbeta))))
}

gradloglikelihood <- function(beta){
  xbeta <- X %*% beta
  Yx <- colSums(Y * X)
  return(Yx - t(X) %*% (1 / (1 + exp(-xbeta))))
}
```

We specify a prior distribution for $\beta\in\mathbb{R}^d$ using a Normal distribution with mean vector $b=0_d$ and covariance matrix $B=10 I_d$. We define a function `rprior` that returns $N\in\mathbb{N}$ random samples from the prior distribution, a function `logprior` that evaluates the log-prior density $\log p(\beta)=\log\mathcal{N}(\beta;b,B)$, and a function `gradlogprior` that evaluates the gradient of the log-prior density (with respect to $\beta$) $\nabla\log p(\beta)=B^{-1}(b-\beta)$. The last two functions take as input a $d\times N$ matrix `beta` containing $N$ sets of regression coefficients. 

```{r prior}
b <- matrix(0, nrow = d, ncol = 1)
B <- diag(10, d, d)

rprior <- function(N){
  return(sqrt(10) * matrix(rnorm(d*N), nrow = d))
}

logprior <- function(beta){
  return(colSums(dnorm(beta, mean = 0, sd = sqrt(10), log = TRUE)))
}

gradlogprior <- function(beta){
  return(-beta / 10)
}
```

# Components of sequential Monte Carlo sampler
We now specify various algorithmic components that form a sequential Monte Carlo sampler (SMCS) [@del2006sequential]. 

## Path of distributions
First, we select a path of distributions $(\pi_t)_{t=0}^T$ with the posterior distribution of $\beta$ as its last element. We consider the geometric path 
$$\pi_t(\beta)=\frac{\gamma_t(\beta)}{Z_t},\quad 
\gamma_t(\beta)= p(\beta)p(y|x,\beta)^{\lambda_t}, \quad
Z_t=\int_{\mathbb{R}^d}\gamma_t(\beta)d\beta,
$$ 
defined by a sequence $0=\lambda_0<\lambda_1<\cdots<\lambda_T=1$. This involves raising the likelihood to a power of $\lambda_t$, usually referred to as the inverse temperature due to connections with simulated annealing [@kirkpatrick1983optimization]. This choice is valid as it recovers the desired posterior distribution of $\beta$ when $\lambda_T=1$, i.e. $\pi_T(\beta)=p(\beta|x,y)$ with normalizing constant $Z_T=p(y|x)$. When $\lambda_0=0$, the initial distribution is the prior distribution of $\beta$, i.e. $\pi_0(\beta)=p(\beta)$ and the normalizing constant is $Z_0=1$. To initialize our SMCS with draws $\beta_0^n\sim\pi_0,n\in[N]$, we can use the `rprior` function defined earlier. 

To evaluate the unnormalized density $\gamma_t(\beta)$ and its gradient in the logarithmic scale, we will rely on the following identities
$$ 
\log \gamma_t(\beta) = \log p(\beta) + \lambda_t \log p(y|x,\beta),\quad
\nabla\log \gamma_t(\beta) = \nabla\log p(\beta) + \lambda_t \nabla\log p(y|x,\beta),
$$ 
and the functions defined above. 

```{r path}
loggamma <- function(beta, lambda) logprior(beta) + lambda * loglikelihood(beta)
gradloggamma <- function(beta, lambda) gradlogprior(beta) + lambda * gradloglikelihood(beta)
```

## Forward Markov kernels
For each $t\in[T]$, we shall select the forward Markov kernel $M_t$ using a Hamiltonian Monte Carlo (HMC) method targeting $\pi_t$. We will focus on its algorithmc implementation and refer readers to @neal2011mcmc for more details about HMC. We first define a function `leapfrog` which performs leapfrog numerical integration to simulate Hamiltonian dynamics. Briefly speaking, this function evolves a given `state` and `velocity` using a function `gradlogtarget` that evaluates $\nabla\log\pi_t(\beta)=\nabla\log\gamma_t(\beta)$. The algorithmic tuning parameters `stepsize` and `nleapfrog` control the size of the time-discretization and the number of integration steps. Although these are fixed for illustrative purposes, one should experiment with various choices and consider adaptive schemes to automated their selection for different target distributions [@buchholz2018adaptive]. 

```{r leapfrog}
stepsize <- 0.1 * d^{-1/4}
nleapfrog <- ceiling(1 / stepsize)

leapfrog <- function(state, velocity, gradlogtarget){
  velocity <- velocity + stepsize * gradlogtarget(state) / 2
  for (i in 1:nleapfrog){
    state <- state + stepsize * velocity
    if (i != nleapfrog){
      velocity <- velocity + stepsize * gradlogtarget(state)
    }
  }
  velocity <- velocity + stepsize * gradlogtarget(state) / 2
  return(list(state = state, velocity = velocity))
}
```
Next, we define a function `hmc_kernel` that samples from the HMC Markov kernel. It proposes new parameters by calling `leapfrog`, and accepts them with a probability that depends on a function `logtarget` that evaluates $\log\gamma_t(\beta)$. 

```{r hmc}
hmc_kernel <- function(beta, logtarget, gradlogtarget){
  N <- ncol(beta)
  
  # sample velocity
  velocity <- matrix(rnorm(d*N), nrow = d, ncol = N) 
  
  # run leap-frog integrator
  leapfrog_result <- leapfrog(beta, velocity, gradlogtarget)
  proposed_beta <- leapfrog_result$state
  proposed_velocity <- leapfrog_result$velocity
  
  # compute acceptance probability
  logtarget_current <- logtarget(beta)
  logtarget_proposal <- logtarget(proposed_beta)
  accept_ratio <- logtarget_proposal - logtarget_current + colSums(velocity^2) / 2 - colSums(proposed_velocity^2) / 2
  finite_logical <- is.finite(accept_ratio) # if there is stability issues
  nfinite <- sum(finite_logical) 
  
  # accept or reject proposal
  accept_logical <- rep(FALSE, N)
  accept_logical[finite_logical] <- (log(runif(nfinite)) < accept_ratio[finite_logical])
  beta[, accept_logical] <- proposed_beta[, accept_logical]
  acceptrate <- sum(accept_logical) / N
  
  return(list(beta = beta, acceptrate = acceptrate))
}
```
Running several iterations of the HMC kernel targeting $\pi_t$ to move samples $(\check{\beta}_{t-1}^n)_{n\in[N]}$ from step $t-1$ to a new set of samples $(\beta_{t}^n)_{n\in[N]}$ approximating $\pi_t$ implicitly defines a $\pi_t$-invariant forward Markov kernel $M_t$, i.e. $\beta_{t}^n\sim M_t(\check{\beta}_{t-1}^n,\cdot)$ for $n\in[N]$.

## Backward Markov kernels and weight functions
We then choose a sequence of backward kernels $(L_t)_{t=0}^{T-1}$ to ensure that importance weights are tractable. We employ the standard choice of having $L_{t-1}$ as the time reversal of $M_t$ [@jarzynski_1997; @neal2001annealed]. This leads to the weight function $w_t(\beta_{t-1})=\gamma_t(\beta_{t-1})/\gamma_{t-1}(\beta_{t-1})$. For numerical stability reasons, we will implement the weight function in the logarithmic scale instead of the natural scale. Using the form of $\gamma_t$, we can write the log-weight function as
$$ 
\log w_t(\beta_{t-1}) = (\lambda_t-\lambda_{t-1}) \log p(y|x,\beta_{t-1}).
$$
This specific expression is implemented below in our definition of `logweight_function`. This function takes as arguments `lambda_current` and `lambda_next` specifying the successive inverse temperatures $\lambda_{t-1}$ and $\lambda_t$, and a $d\times N$ matrix `beta` containing $N$ sets of regression coefficients $\beta_{t-1}^n\in\mathbb{R}^d, n\in[N]$ before the forward Markov kernel $M_t$ is applied. 

```{r logweights}
logweight_function <- function(lambda_current, lambda_next, beta){
  logweights <- (lambda_next - lambda_current) * loglikelihood(beta)
  logweights[is.na(logweights)] <- -Inf
  return(logweights)
}
```

## Weight computations and resampling 
To compute normalized weights $w_t^n\propto w_t(\beta_{t-1}^n), n\in[N]$ satisfying $\sum_{n=1}^Nw_t^n=1$, we apply the log-sum-exp trick to prevent numerical under/over-flow. Our implementation of the `normalize_weights` function also computes the log-mean weights $\log(N^{-1}\sum_{n=1}^N w_t(\beta_{t-1}^n))$, which is an estimator of 
$$
\log Z_t - \log Z_{t-1} =
\log\left(\int_{\mathbb{R}^d}w_t(\beta_{t-1})\pi_{t-1}(d\beta_{t-1})\right).
$$
```{r normweights}
normalize_weights <- function(logweights){
  maxlogweights <- max(logweights)
  weights <- exp(logweights - maxlogweights)
  normweights <- weights / sum(weights)
  logmeanweights <- log(mean(weights)) + maxlogweights
  return(list(weights = normweights, logmean = logmeanweights))
}
```

To measure the variability of the normalized weights $(w_t^n)_{n\in[N]}$, we define a function `compute_ess` to compute the effective sample size (ESS) criterion $\mathrm{ESS}_t=\{\sum_{n=1}^N(w_t^n)^2\}^{-1}$ [@kong1994sequential]. The ESS lies between $1$ to $N$, achieving the lower bound when the weights degenerates to a single sample, and the upper bound in the case of uniform weights. 

```{r ess}
compute_ess <- function(normweights){
  ess <- 1 / sum(normweights^2)
  return(ess)
}
```

To mitigate the issue of weight degeneracy, we perform an operation known as resampling. This allows samples with low weights to be discarded, and samples with large weights to be duplicated. Below, we implement a simple scheme called multinomial resampling, which involves drawing ancestor indices $(a_{t-1}^n)_{n\in[N]}$ independently from the Categorical distribution on $[N]$ with probabilities given by the normalized weights $(w_t^n)_{n\in[N]}$. The resampled particles are then obtained by setting $\check{\beta}_{t-1}^n=\beta_{t-1}^{a_{t-1}^n}$ for $n\in[N]$. 

```{r resampling}
resampling <- function(normweights){
  N <- length(normweights)
  ancestors <- sample(x = 1:N, size = N, prob = normweights, replace = TRUE)
  return(ancestors)
}
```

## Non-adaptive SMCS
We are now in a position to construct an SMCS by assembling the components defined above. We first consider the simple case where we specify the number of bridging distributions $T$ and the linear sequence $\lambda_t=t/T, t\in[T]$. For illustration, we choose $T=50$ steps, $N=200$ samples and the number of HMC iterations `niterations` at each step as one. From some preliminary runs, this requires a few seconds of compute time for this problem. One could experiment with various choices and see how it impacts the algorithmic performance and run-time. 
The algorithm returns a $d\times N$ matrix `particles` containing samples $(\beta_T^n)_{n\in[N]}$ approximating $\pi_T$, a numeric vector `log_normconst` of log-normalizing constant estimates $(\log Z_t^N)_{t\in[T]}$, a numeric vector `ess` of effective sample sizes, and a numeric vector `acceptrate` monitoring the acceptance rate of HMC at each step. 

```{r smc}
# algorithmic tuning parameters
T <- 50
lambda_schedule <- seq(0, 1, length.out = T+1)
N <- 200
niterations <- 1

# initialization
particles <- rprior(N)
log_normconst <- rep(0, T + 1)
log_ratio_normconst <- 0
ess <- rep(0, T + 1)
ess[1] <- N
acceptrate <- rep(0, T)

# intermediate steps
for (t in 1:T){
  # compute log-weights
  logweights <- logweight_function(lambda_schedule[t], lambda_schedule[t+1], particles)
  
  # normalize weights
  normalized <- normalize_weights(logweights)
  normweights <- normalized$weights 
  logmeanweights <- normalized$logmean
  
  # compute effective sample size
  ess[t+1] <- compute_ess(normweights)
  
  # compute normalizing constant estimate
  log_ratio_normconst <- log_ratio_normconst + logmeanweights
  log_normconst[t+1] <- log_ratio_normconst
  
  # multinomial resampling
  ancestors <- resampling(normweights)
  particles <- particles[, ancestors, drop=F]
  
  # HMC iterations  
  logtarget <- function(beta) loggamma(beta, lambda_schedule[t+1])
  gradlogtarget <- function(beta) gradloggamma(beta, lambda_schedule[t+1])
  for (i in 1:niterations){
    hmc_result <- hmc_kernel(particles, logtarget, gradlogtarget)
    particles <- hmc_result$beta
  }
  acceptrate[t] <- hmc_result$acceptrate
}
```

Figures such as histograms and scatter plots of `particles` can be used to visualize our approximation of the posterior distribution of $\beta$.

```{r pairs}
hist(particles[1, ], probability = TRUE, xlab = 'beta0', main = 'Histogram of beta0')
pairs(t(particles))
```

Plotting `log_normconst` allows us to observe the time evolution of log-normalizing constant estimates. The last element is an estimate of the log-marginal likelihood of the data set. 

```{r smc-logz}
plot(0:T, log_normconst, type = 'l', xlab = 'step', ylab = 'estimate')
cat('Log-marginal likelihood estimate:', log_normconst[T+1])
```

Next, we plot the ESS percentage to inspect the degree of weight degeneracy over the time steps. Notice that the ESS drops sharply in the beginning, and remains high for most steps later on. This suggests that we should increase $\lambda_t$ slower than linearly at the start, and faster than linearly towards the end. In the following, we will show how to achieve this property using an adaptive scheme. 

```{r smc-ess}
plot(0:T, ess * 100 / N, type = 'l', xlab = 'step', ylab = 'ESS%', ylim = c(0,100))
```

Lastly, we inspect the acceptance rates of HMC. Low/high acceptance rates suggest that we should decrease/increase the tuning parameter `stepsize` defining `leapfrog` and hence `hmc_kernel`.

```{r smc-acceptrate}
plot(1:T, acceptrate, type = 'l', xlab = 'step', ylab = 'Acceptance rate', ylim = c(0,1))
```

## Adaptive SMCS
We now describe a common procedure that specifies $T$ and $(\lambda_t)_{t\in[T]}$ adaptively. Suppose we are at step $t-1$ and $\pi_{t-1}$ has been determined by some $\lambda_{t-1}\in[0,1)$. To keep the ESS constant over time, we seek $\lambda_t\in(\lambda_{t-1},1]$ so that $\mathrm{ESS}_t(\lambda_t)$ is equal to a desired level $\kappa N$, for some criterion $\kappa\in(0,1)$ that we will specify. 
Below, we implement a function `search_lambda` that performs a binary search on $[\lambda_{t-1},1]$ to solve for $\lambda_t$. It takes as input arguments a numeric value `lambda_current` specifying $\lambda_{t-1}$, a function `ess_function` that allows us to evaluate $\lambda_t\mapsto\mathrm{ESS}_t(\lambda_t)$, and a numeric value `objective` specifying the desired ESS $\kappa N$. 
For efficiency, we should write `ess_function` in a manner that precomputes terms involving the samples $(\beta_{t-1}^n)_{n\in[N]}$ as they do not depend on the choice of $\lambda_t$. Two other optional arguments `maxsteps` and `tolerance` control the maximum number of bisection steps and the error tolerance of the binary search. 

```{r bisection}
# binary search
search_lambda <- function(lambda_current, ess_function, objective, maxsteps = 1000, tolerance = 1e-2){
  if ((ess_function(lambda_current) < objective)|| ess_function(1) > objective){
    print("Problem: there is no solution to the binary search")
  }
  attempt <- 1
  current_size <- (1 - lambda_current)/2
  ess_attempt <- ess_function(attempt)
  istep <- 0
  while (!(ess_attempt >= objective && ess_attempt < objective+tolerance) && (istep < maxsteps)){
    istep <- istep + 1
    if (ess_attempt > objective){
      attempt <- attempt + current_size
      ess_attempt <- ess_function(attempt)
      current_size <- current_size / 2
    } else {
      attempt <- attempt - current_size
      ess_attempt <- ess_function(attempt)
      current_size <- current_size / 2
    }
  }
  lambda_next <- attempt
  return(lambda_next)
}
```

We may now construct an adaptive SMCS by assembling the above components as before. For illustrative purpose, we set the ESS criterion as $\kappa=0.5$, the number of samples as $N=200$ and the number of HMC iterations `niterations` at each step as one. The algorithmic run-time is now random as the number of bridging distributions $T$ is itself random, and dependent on the choice of $\kappa$. One could experiment with various values of $\kappa$ and $N$ to see how it changes the behavior of $T$. The algorithmic outputs `particles`, `log_normconst`, `ess` and `acceptrate` are the same as the non-adaptive sampler. 

```{r asmc}
# algorithmic tuning parameters
kappa <- 0.5
N <- 200
niterations <- 1

# initialization
particles <- rprior(N)
log_ratio_normconst <- 0
log_normconst <- 0
ess <- N
lambda_current <- 0
lambda_schedule <- lambda_current
acceptrate <- c()

# intermediate steps
while (lambda_current < 1){
  # search for next inverse temperature
  loglikelihood_values <- loglikelihood(particles)
  ess_next <- function(lambda){
    logweights_next <- (lambda - lambda_current) * loglikelihood_values
    logweights_next[is.na(logweights_next)] <- -Inf
    normweights_next <- normalize_weights(logweights_next)$weights
    return(compute_ess(normweights_next))
  }
  if (ess_next(1) > kappa * N){
    lambda_next <- 1
  } else {
    lambda_next <- search_lambda(lambda_current, ess_next, kappa * N)
  }
  
  # compute log-weights
  logweights <- logweight_function(lambda_current, lambda_next, particles)
  
  # normalize weights
  normalized <- normalize_weights(logweights)
  normweights <- normalized$weights 
  logmeanweights <- normalized$logmean
  
  # compute effective sample size
  ess <- c(ess, compute_ess(normweights))
  
  # compute normalizing constant
  log_ratio_normconst <- log_ratio_normconst + logmeanweights
  log_normconst <- c(log_normconst, log_ratio_normconst)
  
  # multinomial resampling
  ancestors <- resampling(normweights)
  particles <- particles[, ancestors, drop=F]
  
  # HMC iterations  
  logtarget <- function(beta) loggamma(beta, lambda_next)
  gradlogtarget <- function(beta) gradloggamma(beta, lambda_next)
  for (i in 1:niterations){
    hmc_result <- hmc_kernel(particles, logtarget, gradlogtarget)
    particles <- hmc_result$beta
  }
  acceptrate <- c(acceptrate, hmc_result$acceptrate)
  
  # store lambda schedule
  lambda_current <- lambda_next
  lambda_schedule <- c(lambda_schedule, lambda_current)
}
```

One can inspect `particles` and `log_normconst` to check that we obtain similar results as before. Below, we print the number of bridging distributions $T$ and compare the adaptively-determined sequence $(\lambda_t)_{t\in[T]}$ to the linear schedule. 

```{r adaptive-lambda}
T <- length(lambda_schedule) - 1
cat("Number of bridging distributions:", T)

plot(0:T, lambda_schedule, xlab = 'step', ylab = 'lambda')
lines(0:T, seq(0, 1, length.out = T+1), col = 'red')
```

Lastly, as a sanity check, we plot the ESS percentage to verify that it is maintained at the desired level of $\kappa=50\%$, with the exception of the initial and terminal steps. 

```{r adaptive-ess}
plot(0:T, ess * 100 / N, type = 'l', xlab = 'step', ylab = 'ESS%', ylim = c(0,100))
```

# References
